{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zjfx7sQJBFA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pdfplumber\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vjLLLzu3JYzn"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    extracted_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            extracted_text += page.extract_text() + \"\\n\"\n",
        "    return extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DDHna7BZJeCJ"
      },
      "outputs": [],
      "source": [
        "# Step 2: Split text into chunks\n",
        "def split_into_chunks(text, chunk_size=100):\n",
        "    sentences = text.split(\". \")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk.split()) + len(sentence.split()) <= chunk_size:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WMv6c6L8JlB-"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create embeddings for the chunks\n",
        "def create_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks)\n",
        "    return embeddings, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oKUnFDIXJs65"
      },
      "outputs": [],
      "source": [
        "# Step 4: Store embeddings in FAISS\n",
        "\n",
        "def store_embeddings(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9gQxBNazJxsS"
      },
      "outputs": [],
      "source": [
        "# Step 5: Search the most relevant chunks\n",
        "def search_query(query, index, chunks, model):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, k=1)\n",
        "\n",
        "    results = []\n",
        "    for i in indices[0]:\n",
        "        results.append(chunks[i])\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T-koDNulJ3gW"
      },
      "outputs": [],
      "source": [
        "# Step 6: Use a mock LLM to generate a response\n",
        "def generate_response(query, results):\n",
        "    response = \"Here are the relevant results for your query: \\n\\n\"\n",
        "    for result in results:\n",
        "        response += f\"- {result}\\n\\n\"\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRiJeWFyKAKn",
        "outputId": "83a66956-5b72-44d9-c3a9-3b9a29813748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the relevant results for your query: \n",
            "\n",
            "- Tables, Charts, and\n",
            "Graphs\n",
            "with Examples from History, Economics,\n",
            "Education, Psychology, Urban Affairs and\n",
            "Everyday Life\n",
            "REVISED: MICHAEL LOLKUS 2018\n",
            "\n",
            "Tables, Charts, and\n",
            "Graphs Basics\n",
            " We use charts and graphs to visualize data.\n",
            " This data can either be generated data, data gathered from\n",
            "an experiment, or data collected from some source.\n",
            " A picture tells a thousand words so it is not a surprise that\n",
            "many people use charts and graphs when explaining data.\n",
            "Types of Visual\n",
            "Representations of Data\n",
            "Table of Yearly U.S. GDP by\n",
            "Industry (in millions of dollars)\n",
            "Source: U.S.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Main Script\n",
        "def main():\n",
        "    # 1. Path to the PDF\n",
        "    pdf_path = \"test.pdf\"  # Replace with your PDF path\n",
        "\n",
        "    # 2. Extract and preprocess text\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = split_into_chunks(text)\n",
        "\n",
        "    # 3. Create embeddings\n",
        "    embeddings, model = create_embeddings(chunks)\n",
        "\n",
        "    # 4. Store embeddings in FAISS index\n",
        "    index = store_embeddings(np.array(embeddings))\n",
        "\n",
        "    # 5. Query the system\n",
        "    query = \"from page 6 get the tabular data\"  # Replace with your query\n",
        "    results = search_query(query, index, chunks, model)\n",
        "\n",
        "    # 6. Generate and print response\n",
        "    response = generate_response(query, results)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
